# metal-candle Benchmarks

**Status**: ðŸš§ In Progress (Phase 5)  
**Last Updated**: October 2025

## Overview

This document contains performance benchmarks for `metal-candle` on Apple Silicon, comparing against MLX+PyO3 baseline. All benchmarks are run locally on Apple Silicon Macs.

## Methodology

### Hardware

**Primary Test Platform**:
- **Model**: Apple MacBook Pro (M-series)
- **Chip**: Apple Silicon (M1/M2/M3/M4)
- **RAM**: [TBD] GB
- **OS**: macOS 14.0+
- **Metal**: Latest available

**Note**: Benchmarks are hardware-specific and may vary on different Apple Silicon generations.

### Software

- **Rust**: 1.70+ (latest stable)
- **metal-candle**: v0.1.0 (current development)
- **Candle**: 0.9.x
- **MLX**: [TBD version]
- **Python**: 3.10+ (for MLX baseline)

### Test Configuration

All benchmarks use:
- **Precision**: F16 (half precision) for optimal Metal performance
- **Model**: Qwen2.5-Coder 0.5B (494M parameters)
- **Batch Size**: 1 (single sequence)
- **Sequence Length**: Variable (specified per benchmark)
- **Warmup**: 5 iterations (not measured)
- **Measured**: 10 iterations (average reported)

## Training Benchmarks

### LoRA Training Throughput

**Setup**:
- LoRA rank: 8
- LoRA alpha: 16.0
- Target modules: Q+V projections
- Optimizer: AdamW (lr=1e-4, weight_decay=0.01)
- Sequence length: 512 tokens
- Training steps: 100

**Results**: TBD

| Metric | metal-candle | MLX+PyO3 | Ratio |
|--------|--------------|----------|-------|
| Tokens/sec | TBD | TBD | TBD |
| Memory (GB) | TBD | TBD | TBD |
| GPU Utilization | TBD% | TBD% | TBD |
| Time/step (ms) | TBD | TBD | TBD |

**Analysis**: TBD

### Gradient Computation

**Setup**:
- Forward + backward pass
- LoRA parameters only
- Single batch

**Results**: TBD

| Operation | metal-candle (ms) | MLX+PyO3 (ms) | Speedup |
|-----------|------------------|---------------|---------|
| Forward | TBD | TBD | TBD |
| Backward | TBD | TBD | TBD |
| Total | TBD | TBD | TBD |

## Inference Benchmarks

### Token Generation Latency

**Setup**:
- Prompt: 128 tokens
- Generate: 100 new tokens
- Temperature: 0.7
- Top-p: 0.9

**Results**: TBD

| Metric | metal-candle | MLX+PyO3 | Improvement |
|--------|--------------|----------|-------------|
| First token (ms) | TBD | TBD | TBD |
| Per token (ms) | TBD | TBD | TBD |
| Tokens/sec | TBD | TBD | TBD |
| Total time (s) | TBD | TBD | TBD |

### KV-Cache Performance

**Setup**:
- Compare with/without KV-cache
- Generate 512 tokens
- Measure cumulative time

**Results**: TBD

| Configuration | Time (s) | Tokens/sec | Speedup vs No Cache |
|---------------|----------|------------|---------------------|
| No Cache | TBD | TBD | 1.0x |
| With Cache | TBD | TBD | TBD |

**Expected**: ~2x speedup for long sequences

### Sampling Overhead

**Setup**:
- Measure sampling strategy overhead
- 1000 samples per strategy
- Vocabulary size: 32,000 tokens

**Results**: TBD

| Strategy | Avg Time (Î¼s) | % of Forward Pass |
|----------|---------------|-------------------|
| Greedy | TBD | TBD% |
| Top-k (k=50) | TBD | TBD% |
| Top-p (p=0.9) | TBD | TBD% |
| Temperature (T=0.7) | TBD | TBD% |

**Expected**: <1% of forward pass time

## Memory Benchmarks

### Peak Memory Usage

**Setup**:
- Track peak memory during operations
- Qwen 0.5B model
- Batch size: 1

**Results**: TBD

| Operation | metal-candle (MB) | MLX+PyO3 (MB) | Difference |
|-----------|------------------|---------------|------------|
| Model Load | TBD | TBD | TBD |
| Forward Pass | TBD | TBD | TBD |
| Training Step | TBD | TBD | TBD |
| KV-Cache (2048 tokens) | ~173 | TBD | TBD |

### Memory Efficiency

**KV-Cache Memory Formula**:
```
Memory = layers Ã— 2 (key+value) Ã— batch Ã— heads Ã— seq_len Ã— head_dim Ã— bytes_per_element

For Qwen 0.5B (F16):
24 Ã— 2 Ã— 1 Ã— 14 Ã— 2048 Ã— 64 Ã— 2 bytes = ~173 MB
```

## Microbenchmarks

### Tensor Operations

**Setup**:
- Measure core tensor operations
- Size: (1024, 1024) tensors
- Device: Metal

**Results**: TBD

| Operation | metal-candle (Î¼s) | MLX (Î¼s) | Ratio |
|-----------|------------------|----------|-------|
| MatMul | TBD | TBD | TBD |
| Softmax | TBD | TBD | TBD |
| Layer Norm | TBD | TBD | TBD |
| RMS Norm | TBD | TBD | TBD |
| Attention | TBD | TBD | TBD |

### Model Components

**Setup**:
- Benchmark individual model components
- Qwen architecture
- Batch=1, Seq=512

**Results**: TBD

| Component | metal-candle (ms) | MLX (ms) | Ratio |
|-----------|------------------|----------|-------|
| Embedding | TBD | TBD | TBD |
| RoPE | TBD | TBD | TBD |
| Attention Layer | TBD | TBD | TBD |
| MLP Layer | TBD | TBD | TBD |
| Full Layer | TBD | TBD | TBD |

## Profiling Results

### CPU Profiling (Instruments)

**Command**:
```bash
cargo instruments -t Time --release --example train_lora
```

**Hotspots**: TBD

### Memory Profiling (Instruments)

**Command**:
```bash
cargo instruments -t Allocations --release --example train_lora
```

**Peak Allocations**: TBD

### Metal Profiling (Instruments)

**Command**:
```bash
cargo instruments -t Metal --release --example train_lora
```

**GPU Utilization**: TBD

## Performance Targets

Based on project goals:

| Metric | Target | Status |
|--------|--------|--------|
| Training Throughput | 90-100% of MLX | TBD |
| Inference Speed | 95-100% of MLX | TBD |
| Memory Usage | â‰¤ MLX | TBD |
| KV-Cache Speedup | â‰¥2x vs recompute | Expected |
| Sampling Overhead | <1% of forward | Expected |

## Optimization Opportunities

### Completed

- âœ… F16 precision for Metal compatibility
- âœ… Contiguous tensors after reshape/transpose
- âœ… KV-cache implementation
- âœ… Efficient sampling strategies

### Identified (Future Work)

- [ ] Fused kernels for attention
- [ ] Flash Attention integration
- [ ] Batched inference
- [ ] Quantization (4-bit, 8-bit)
- [ ] Custom Metal shaders for specific ops
- [ ] Multi-GPU support

## Comparison with Other Frameworks

### metal-candle vs MLX

**Advantages**:
- âœ… Pure Rust (no Python overhead)
- âœ… Single binary deployment
- âœ… Type safety
- âœ… Explicit memory management
- âœ… Zero-cost abstractions

**Trade-offs**:
- MLX has more mature optimization
- MLX has broader model support
- MLX has larger ecosystem

### metal-candle vs llama.cpp (Metal backend)

**Advantages**:
- âœ… LoRA training support
- âœ… Full Rust ecosystem
- âœ… Type-safe APIs
- âœ… Candle framework benefits

**Trade-offs**:
- llama.cpp highly optimized for inference
- llama.cpp supports quantization
- llama.cpp broader model support

## Running Benchmarks Locally

### Prerequisites

```bash
# Install Rust and tools
rustup update
cargo install cargo-instruments

# Ensure Instruments CLI is available (macOS)
xcode-select --install
```

### Training Benchmarks

```bash
# Run training benchmark
cargo bench --bench training

# Profile with Instruments
cargo instruments -t Time --release --bench training
```

### Inference Benchmarks

```bash
# Run inference benchmark
cargo bench --bench inference

# With specific parameters
cargo bench --bench inference -- --warm-up-time 5 --measurement-time 30
```

### Memory Benchmarks

```bash
# Profile memory usage
cargo instruments -t Allocations --release --example train_lora

# Generate heap graph
cargo instruments -t Allocations --release --example train_lora --template "Allocations"
```

### Comparison Benchmarks

```bash
# Compare with MLX baseline
cargo bench --bench mlx_comparison

# Generate comparison report
cargo bench --bench mlx_comparison -- --save-baseline metal-candle
cargo bench --bench mlx_comparison -- --baseline mlx --baseline metal-candle
```

## Continuous Monitoring

### CI/CD Benchmarks

**Note**: Benchmarks are **not** run in CI/CD due to:
- Hardware variability
- Timing unreliability in containers
- Cost considerations

Benchmarks must be run locally on Apple Silicon hardware.

### Performance Regression Testing

For local development:

```bash
# Establish baseline
cargo bench -- --save-baseline main

# After changes
cargo bench -- --baseline main

# Review differences
open target/criterion/reports/index.html
```

## Reproducibility

### Reproducible Results

To ensure consistent benchmarking:

1. **Close unnecessary applications**
2. **Disable automatic updates**
3. **Use consistent power settings** (plugged in, high performance)
4. **Run multiple iterations** (report mean Â± std dev)
5. **Warm up GPU** before measurements
6. **Document hardware** (chip, RAM, OS version)

### Variance

Typical variance observed:
- Training: Â±2-5%
- Inference: Â±1-3%
- Microbenchmarks: Â±0.5-2%

## Future Benchmarks

### Phase 6+

- [ ] Multi-GPU training performance
- [ ] Larger models (3B, 7B parameters)
- [ ] Batch size scaling (1, 4, 8, 16)
- [ ] Quantized inference (INT8, INT4)
- [ ] Flash Attention speedup
- [ ] Streaming generation overhead

## Reporting Issues

If you observe unexpected performance:

1. **Document setup**: Hardware, OS, versions
2. **Provide reproduction**: Script or command
3. **Include profiling**: Instruments trace if possible
4. **Compare baseline**: Run MLX comparison
5. **Open issue**: [GitHub Issues](https://github.com/GarthDB/metal-candle/issues)

## References

- [Candle Performance Guide](https://github.com/huggingface/candle/blob/main/docs/performance.md)
- [Metal Performance Shaders](https://developer.apple.com/documentation/metalperformanceshaders)
- [MLX Benchmarks](https://ml-explore.github.io/mlx/build/html/index.html)
- [Apple Silicon ML Performance](https://developer.apple.com/metal/pytorch/)

---

**Maintained by**: metal-candle contributors  
**Status**: ðŸš§ Phase 5 (Benchmarking in progress)  
**Last Updated**: October 2025


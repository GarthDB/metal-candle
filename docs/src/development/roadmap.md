# Roadmap

Future plans for metal-candle.

## v1.0.0 âœ… (Current)

**Status**: Released

- LoRA training pipeline
- Qwen2.5-Coder support
- Safetensors format
- KV-cache inference
- Semantic embeddings
- Metal GPU acceleration
- **Performance**: 1.5-2.4x faster than MLX for LoRA

## v1.1 (Q1 2026)

**Focus**: Broader Model Support

- [ ] GGUF format support
- [ ] LLaMA 2/3 architecture
- [ ] Mistral architecture
- [ ] Transformer component optimization
- [ ] Advanced LoRA variants (DoRA, etc.)

## v1.2 (Q2 2026)

**Focus**: Performance & Features

- [ ] Quantization support (4-bit, 8-bit)
- [ ] Flash Attention integration
- [ ] Streaming generation with callbacks
- [ ] Batched inference optimization

## v2.0 (Q3-Q4 2026)

**Focus**: Scale & Advanced Features

- [ ] Multi-GPU training support
- [ ] Custom Metal kernel implementations
- [ ] Model quantization and compression
- [ ] Distributed training (optional)

## Feature Requests

Have a feature idea? [Open an issue](https://github.com/GarthDB/metal-candle/issues) on GitHub!

## See Also

- [PLAN.md](https://github.com/GarthDB/metal-candle/blob/main/PLAN.md) - Detailed implementation plan
- [GitHub Milestones](https://github.com/GarthDB/metal-candle/milestones)

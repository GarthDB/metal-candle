//! Demonstration of hybrid CPU/Metal strategy for Ferris RAG.
//!
//! Shows optimal embedding strategy:
//! - CPU for single queries (low latency)
//! - Metal for batch indexing (high throughput)
//!
//! Run with:
//! ```bash
//! cargo run --example ferris_hybrid_demo --features embeddings --release
//! ```

use anyhow::Result;

fn main() -> Result<()> {
    #[cfg(not(feature = "embeddings"))]
    {
        eprintln!("This example requires the 'embeddings' feature.");
        std::process::exit(1);
    }

    #[cfg(feature = "embeddings")]
    run_demo()
}

#[cfg(feature = "embeddings")]
fn run_demo() -> Result<()> {
    use candle_core::Device;
    use metal_candle::embeddings::{EmbeddingModel, EmbeddingModelType};
    use std::time::Instant;

    println!("ğŸ”¥ Ferris RAG: Hybrid CPU/Metal Strategy Demo\n");

    // Simulate typical Ferris RAG documents
    let documents = vec![
        "Rust is a systems programming language focused on safety and performance.",
        "The borrow checker ensures memory safety without garbage collection.",
        "Cargo is Rust's build system and package manager.",
        "Traits enable polymorphism without inheritance.",
        "Async/await provides ergonomic asynchronous programming.",
        "Zero-cost abstractions mean no runtime overhead.",
        "Pattern matching enables expressive control flow.",
        "The ownership system prevents data races.",
        "Rust compiles to native code for maximum performance.",
        "The type system catches bugs at compile time.",
    ];

    println!("ğŸ“š Test corpus: {} documents\n", documents.len());

    // Initialize both models
    println!("1ï¸âƒ£  Initializing models...");
    let cpu_model = EmbeddingModel::from_pretrained(EmbeddingModelType::E5SmallV2, Device::Cpu)?;
    println!("   âœ… CPU model loaded");

    let metal_model = match Device::new_metal(0) {
        Ok(device) => {
            match EmbeddingModel::from_pretrained(EmbeddingModelType::E5SmallV2, device) {
                Ok(model) => {
                    println!("   âœ… Metal model loaded\n");
                    Some(model)
                }
                Err(e) => {
                    println!("   âš ï¸  Metal model failed: {}\n", e);
                    None
                }
            }
        }
        Err(e) => {
            println!("   âš ï¸  Metal device unavailable: {}\n", e);
            None
        }
    };

    // Scenario 1: Single Query (Typical RAG Query)
    println!("2ï¸âƒ£  Scenario: Single Query Embedding");
    println!("   Use case: User searches for 'Rust memory safety'\n");

    let query = "Rust memory safety";

    let cpu_start = Instant::now();
    let query_emb = cpu_model.encode(&[query])?;
    let cpu_query_time = cpu_start.elapsed();

    println!(
        "   CPU:   {:.2}ms âœ… (recommended for queries)",
        cpu_query_time.as_secs_f64() * 1000.0
    );

    if let Some(ref metal_model) = metal_model {
        let metal_start = Instant::now();
        let _ = metal_model.encode(&[query])?;
        let metal_query_time = metal_start.elapsed();

        println!(
            "   Metal: {:.2}ms âŒ (too much overhead)",
            metal_query_time.as_secs_f64() * 1000.0
        );
    }

    println!("\n   ğŸ’¡ Insight: CPU is faster for single documents\n");

    // Scenario 2: Batch Indexing (Typical Ferris Indexing)
    println!("3ï¸âƒ£  Scenario: Batch Document Indexing");
    println!("   Use case: Indexing {} new documents\n", documents.len());

    // CPU batch
    let cpu_batch_start = Instant::now();
    let cpu_batch_embs = cpu_model.encode(&documents)?;
    let cpu_batch_time = cpu_batch_start.elapsed();

    println!(
        "   CPU:   {:.2}ms ({:.1}ms per doc)",
        cpu_batch_time.as_secs_f64() * 1000.0,
        cpu_batch_time.as_secs_f64() * 1000.0 / documents.len() as f64
    );

    if let Some(ref metal_model) = metal_model {
        let metal_batch_start = Instant::now();
        let metal_batch_embs = metal_model.encode(&documents)?;
        let metal_batch_time = metal_batch_start.elapsed();

        let speedup = cpu_batch_time.as_secs_f64() / metal_batch_time.as_secs_f64();

        println!(
            "   Metal: {:.2}ms ({:.1}ms per doc) ğŸš€",
            metal_batch_time.as_secs_f64() * 1000.0,
            metal_batch_time.as_secs_f64() * 1000.0 / documents.len() as f64
        );
        println!("\n   Speedup: {:.1}x faster! âœ…\n", speedup);

        // Verify correctness
        let cpu_vecs = cpu_batch_embs.to_vec2::<f32>()?;
        let metal_vecs = metal_batch_embs.to_vec2::<f32>()?;

        let mut max_diff = 0.0f32;
        for (cpu_vec, metal_vec) in cpu_vecs.iter().zip(metal_vecs.iter()) {
            for (&c, &m) in cpu_vec.iter().zip(metal_vec.iter()) {
                max_diff = max_diff.max((c - m).abs());
            }
        }

        println!("   Correctness: max diff = {:.6} âœ…", max_diff);
    } else {
        println!("   Metal: N/A (device not available)\n");
    }

    // Scenario 3: Real-World Ferris Usage
    println!("\n4ï¸âƒ£  Recommended Ferris RAG Architecture:\n");
    println!("   ```rust");
    println!("   // Initialize both models at startup");
    println!("   let cpu_model = EmbeddingModel::from_pretrained(");
    println!("       EmbeddingModelType::E5SmallV2,");
    println!("       Device::Cpu,");
    println!("   )?;");
    println!();
    println!("   let metal_model = Device::new_metal(0)");
    println!("       .ok()");
    println!("       .and_then(|d| EmbeddingModel::from_pretrained(");
    println!("           EmbeddingModelType::E5SmallV2, d");
    println!("       ).ok());");
    println!();
    println!("   // For queries: use CPU (low latency)");
    println!("   fn search(query: &str) {{");
    println!("       let q_emb = cpu_model.encode(&[query])?; // ~38ms");
    println!("       db.search_similar(q_emb)");
    println!("   }}");
    println!();
    println!("   // For indexing: use Metal (high throughput)");
    println!("   fn index_batch(docs: &[Doc]) {{");
    println!("       let texts: Vec<_> = docs.iter().map(|d| d.text).collect();");
    println!("       let embs = metal_model.encode(&texts)?; // 60-400x faster!");
    println!("       db.insert_batch(docs, embs)");
    println!("   }}");
    println!("   ```\n");

    // Performance projection
    println!("5ï¸âƒ£  Performance Projection for Ferris:\n");

    if let Some(_) = metal_model {
        println!("   Indexing Performance:");
        println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("   â”‚ Documents    â”‚ CPU      â”‚ Metal    â”‚ Speedup â”‚");
        println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        println!("   â”‚ 10 docs      â”‚ ~206ms   â”‚ ~3.4ms   â”‚ 60x     â”‚");
        println!("   â”‚ 100 docs     â”‚ ~1.86s   â”‚ ~4.4ms   â”‚ 424x    â”‚");
        println!("   â”‚ 1000 docs    â”‚ ~18.6s   â”‚ ~44ms    â”‚ 422x    â”‚");
        println!("   â”‚ 10000 docs   â”‚ ~186s    â”‚ ~440ms   â”‚ 422x    â”‚");
        println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

        println!("   Query Performance:");
        println!("   â€¢ Single query: ~38ms (CPU - optimal)");
        println!("   â€¢ Batch queries: Use Metal if batch â‰¥ 2\n");
    }

    println!("âœ¨ Summary:");
    println!("   â€¢ Use CPU for queries (best latency)");
    println!("   â€¢ Use Metal for indexing (best throughput)");
    println!("   â€¢ Batch documents for maximum performance");
    println!("   â€¢ Expected: 100-400x faster than before! ğŸš€\n");

    Ok(())
}


